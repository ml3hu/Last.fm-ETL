import pandas as pd
import hashlib
from datetime import datetime
import datetime
import json
import pandas as pd
import sqlalchemy
from dotenv import load_dotenv
import os

# read from raw data
def read_raw(today):
    total_pages = 99999 # dummy value
    results = []

    # read in data from json files
    for i in range(1, total_pages):
        if i == 1:
            print("Reading in page " + str(i))
        else:
            print("Reading in page " + str(i) + " of " + str(total_pages))
        
        # change file path to match file path assigned in extract_init.py and extract_update.py
        f = open("/home/ml3hu/Documents/Last.fm-ETL/dags/raw/" + str(today) + " page" + str(i) + ".json")
        data = json.load(f)
        results.append(data)
        f.close()

        # get total pages from first page
        if i == 1:
            total_pages = int(data["recenttracks"]["@attr"]["totalPages"])
        
        # break loop if last page
        # this is necessary because range(1, totalPages) is not recalculated per iteration
        if i == total_pages:
            break

    # normalize and flatten data data
    tracks = [pd.DataFrame(pd.json_normalize(data["recenttracks"]["track"])) for data in results]
    tracks = pd.concat(tracks)

    return tracks



# transform data to match date, time, track, artist group dimensions, and fact table
def transform_data(df):
    print("Transforming Datetime Data")

    df = df.copy()
    #convert timezones
    df["date"] = pd.to_datetime(df["date.#text"]).dt.tz_localize('gmt')
    df["date"] = pd.to_datetime(df["date"]).dt.tz_convert('America/New_York') # change to local timezone

    # transform time data
    df["time"] = pd.to_datetime(df["date"]).dt.time
    # time key generated by concatenation of time
    df["time_of_day_key"] = df["time"].astype(str).str.replace(":", "").astype(int)
    df["hour"] = pd.to_datetime(df["date"]).dt.hour


    # transform date data
    df["date"] = pd.to_datetime(df["date"]).dt.date
    # date key generated by concatenation of date
    df["date_key"] = df["date"].astype(str).str.replace("-", "").astype(int)
    df["year"] = pd.to_datetime(df["date"]).dt.year
    df["month"] = pd.to_datetime(df["date"]).dt.month
    df["day"] = pd.to_datetime(df["date"]).dt.day
    df["day_of_week"] = pd.to_datetime(df["date"]).dt.dayofweek
    
    # transform track data
    df = df.rename(columns={"name": "track_name", "album.#text": "album_name"})
    # md5 hash of track name and album name for track_key https://docs.getdbt.com/blog/sql-surrogate-keys
    df["track_key"] = df["track_name"] + df["album_name"]
    df["track_key"] = [hashlib.md5(key.encode('utf-8')).hexdigest() for key in df["track_key"]]


    # transform artist group data
    df = df.rename(columns={"artist.#text": "artist_group_name"})
    # key generated through md5 hash of artist name
    df["artist_group_key"] = [hashlib.md5(key.encode('utf-8')).hexdigest() for key in df["artist_group_name"]]
    
    return df

# transform to match artist dimension
def transform_artist_data(df):
    # transform artist group to individual artists
    df = df[["artist_group_name"]].copy()
    df = df["artist_group_name"].str.split(", ")
    df = df.explode("artist_group_name").to_frame()
    df = df.drop_duplicates(ignore_index=True)
    df = df.rename(columns={"artist_group_name": "artist_name"})

    # key generated through md5 hash of artist name
    df["artist_key"] = [hashlib.md5(key.encode('utf-8')).hexdigest() for key in df["artist_name"]]

    return df

# create bridge table between artist group and artist
def build_bridge(df, artists):
    # assign artist group key to each participating artist in group
    df = df[["artist_group_key", "artist_group_name"]].copy()
    df["artist_group_name"] = df["artist_group_name"].str.split(", ")
    df = df.explode("artist_group_name")

    # merge bridge table with artist dimension to get artist key per artist group
    df = df.merge(artists, how="left", left_on="artist_group_name", right_on="artist_name")
    df = df.drop(columns=["artist_group_name", "artist_name"])
    df = df.drop_duplicates(ignore_index=True)

    # select bridge table columns
    df = df[["artist_group_key", "artist_key"]]

    return df

# transform data to match dimensions and fact table, then load to staging tables
def transform():
    print("Transforming Data")
    today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

    # read in raw data
    tracks = read_raw(today)

    # transform record data
    tracks = transform_data(tracks)

    # build dimension tables
    print("Building Dimension Tables")
    date_dim = tracks[["date_key", "date", "day", "month", "year", "day_of_week"]].drop_duplicates(ignore_index=True)
    time_of_day_dim = tracks[["time_of_day_key", "time", "hour"]].drop_duplicates(ignore_index=True)
    track_dim = tracks[["track_key", "track_name", "album_name"]].drop_duplicates(ignore_index=True)
    artist_group_dim = tracks[["artist_group_key", "artist_group_name"]].drop_duplicates(ignore_index=True)
    
    # individual artist data + artist group bridge need to be transformed indepentently 
    # because the data does not have a direct relationship to the fact table
    artist_dim = transform_artist_data(tracks)
    artist_group_bridge = build_bridge(tracks, artist_dim)

    # build fact table
    listening_fact = tracks[["date_key", "time_of_day_key", "track_key", "artist_group_key"]]

    # get env variables
    load_dotenv()
    stage_location = os.getenv("STAGE_LOCATION")

    # load to staging area
    print("Loading to Staging Area")

    # connect to db engine
    engine = sqlalchemy.create_engine(stage_location)

    # load data to tables
    try:
        date_dim.to_sql("date_dim", engine, index=False, if_exists='append')
        time_of_day_dim.to_sql("time_of_day_dim", engine, index=False, if_exists='append')
        track_dim.to_sql("track_dim", engine, index=False, if_exists='append')
        artist_dim.to_sql("artist_dim", engine, index=False, if_exists='append')
        artist_group_dim.to_sql("artist_group_dim", engine, index=False, if_exists='append')
        artist_group_bridge.to_sql("artist_group_bridge", engine, index=False, if_exists='append')
        listening_fact.to_sql("listening_fact", engine, index=False, if_exists='append')
    except Exception as e:
        # raise exception if error occurs
        raise e

    print("Data staged successfully")
